{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b1861da4-291a-4717-946a-ca9e762dfa22",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Overview: Databricks Assistant Skills"
    }
   },
   "source": [
    "# Sample Databricks Assistant Skill\n",
    "\n",
    "This notebook demonstrates how to create a custom Databricks Assistant Skill. Skills are modular knowledge packages that extend the Assistant's capabilities in specific domains.\n",
    "\n",
    "## What is a Skill?\n",
    "\n",
    "A Skill is a collection of:\n",
    "* **SKILL.md** - Main documentation with YML frontmatter defining scope and guidance\n",
    "* **README.md** - Overview and usage instructions\n",
    "* **Code examples** - Reusable code snippets and patterns\n",
    "* **Scripts** - Pre-tested utility scripts for common tasks\n",
    "\n",
    "## Skill Structure\n",
    "\n",
    "```\n",
    "skills/\n",
    "└── data-quality-checks/\n",
    "    ├── SKILL.md              # Main skill file with YML frontmatter\n",
    "    ├── README.md             # Overview and instructions\n",
    "    ├── examples/\n",
    "    │   ├── basic-checks.py   # Code examples\n",
    "    │   └── advanced-checks.py\n",
    "    └── scripts/\n",
    "        └── run-checks.sh     # Executable scripts\n",
    "```\n",
    "\n",
    "## Example Use Case: Data Quality Checks\n",
    "\n",
    "This sample demonstrates a **Data Quality Checks** skill that helps users validate data completeness, detect duplicates, and implement quality monitoring."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3b0cdbe2-7bfc-4906-8abb-97184fb137e4",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "SKILL.md - Main Skill Definition"
    }
   },
   "source": [
    "## SKILL.md File\n",
    "\n",
    "The SKILL.md file is the core of your skill. It contains YML frontmatter and detailed guidance.\n",
    "\n",
    "---\n",
    "\n",
    "```markdown\n",
    "---\n",
    "name: data-quality-checks\n",
    "version: 1.0.0\n",
    "description: |\n",
    "  Comprehensive data quality validation for Delta tables. Use when users need to:\n",
    "  - Validate data completeness and accuracy\n",
    "  - Check for null values, duplicates, or outliers\n",
    "  - Implement data quality rules and constraints\n",
    "  - Generate data quality reports\n",
    "  - Set up automated quality monitoring\n",
    "author: Field Engineering\n",
    "tags:\n",
    "  - data-quality\n",
    "  - validation\n",
    "  - delta-lake\n",
    "  - monitoring\n",
    "related_skills:\n",
    "  - data-sampling\n",
    "  - writing-sql\n",
    "last_updated: 2025-02-10\n",
    "---\n",
    "\n",
    "# Data Quality Checks Skill\n",
    "\n",
    "## Scope\n",
    "\n",
    "This skill provides guidance for implementing comprehensive data quality checks on Delta tables in Databricks.\n",
    "\n",
    "## When to Use This Skill\n",
    "\n",
    "Load this skill when:\n",
    "* User asks to validate data quality\n",
    "* User needs to check for missing or invalid data\n",
    "* User wants to implement data quality rules\n",
    "* User needs to generate quality reports\n",
    "* User is setting up data quality monitoring\n",
    "\n",
    "## Core Principles\n",
    "\n",
    "1. **Start with Schema Validation**: Always verify schema before data validation\n",
    "2. **Use Delta Lake Features**: Leverage CHECK constraints and expectations\n",
    "3. **Incremental Validation**: For large tables, validate incrementally\n",
    "4. **Document Quality Rules**: Keep quality rules in version control\n",
    "5. **Automate Monitoring**: Set up alerts for quality issues\n",
    "\n",
    "## Common Patterns\n",
    "\n",
    "### Pattern 1: SQL Completeness Checks\n",
    "\n",
    "```sql\n",
    "-- Check for null values in critical columns\n",
    "SELECT \n",
    "  'user_id' as column_name,\n",
    "  COUNT(*) as total_rows,\n",
    "  SUM(CASE WHEN user_id IS NULL THEN 1 ELSE 0 END) as null_count,\n",
    "  ROUND(100.0 * SUM(CASE WHEN user_id IS NULL THEN 1 ELSE 0 END) / COUNT(*), 2) as null_percentage\n",
    "FROM catalog.schema.users\n",
    "\n",
    "UNION ALL\n",
    "\n",
    "SELECT \n",
    "  'email' as column_name,\n",
    "  COUNT(*) as total_rows,\n",
    "  SUM(CASE WHEN email IS NULL THEN 1 ELSE 0 END) as null_count,\n",
    "  ROUND(100.0 * SUM(CASE WHEN email IS NULL THEN 1 ELSE 0 END) / COUNT(*), 2) as null_percentage\n",
    "FROM catalog.schema.users;\n",
    "```\n",
    "\n",
    "### Pattern 2: Duplicate Detection in SQL\n",
    "\n",
    "```sql\n",
    "-- Find duplicate records based on key columns\n",
    "WITH duplicates AS (\n",
    "  SELECT \n",
    "    user_id,\n",
    "    email,\n",
    "    COUNT(*) as duplicate_count\n",
    "  FROM catalog.schema.users\n",
    "  GROUP BY user_id, email\n",
    "  HAVING COUNT(*) > 1\n",
    ")\n",
    "SELECT \n",
    "  d.user_id,\n",
    "  d.email,\n",
    "  d.duplicate_count,\n",
    "  u.*\n",
    "FROM duplicates d\n",
    "JOIN catalog.schema.users u \n",
    "  ON d.user_id = u.user_id \n",
    "  AND d.email = u.email\n",
    "ORDER BY d.duplicate_count DESC, d.user_id;\n",
    "```\n",
    "\n",
    "### Pattern 3: Delta Lake CHECK Constraints\n",
    "\n",
    "```sql\n",
    "-- Add CHECK constraints to enforce data quality at write time\n",
    "ALTER TABLE catalog.schema.users \n",
    "ADD CONSTRAINT valid_email CHECK (email LIKE '%@%.%');\n",
    "\n",
    "ALTER TABLE catalog.schema.transactions \n",
    "ADD CONSTRAINT positive_amount CHECK (amount > 0);\n",
    "\n",
    "ALTER TABLE catalog.schema.events \n",
    "ADD CONSTRAINT valid_date CHECK (event_date <= current_date());\n",
    "\n",
    "-- View existing constraints\n",
    "SHOW TBLPROPERTIES catalog.schema.users;\n",
    "```\n",
    "\n",
    "### Pattern 4: Data Quality Summary Report\n",
    "\n",
    "```sql\n",
    "-- Comprehensive quality metrics for a table\n",
    "WITH table_stats AS (\n",
    "  SELECT \n",
    "    COUNT(*) as total_rows,\n",
    "    COUNT(DISTINCT user_id) as unique_users,\n",
    "    SUM(CASE WHEN user_id IS NULL THEN 1 ELSE 0 END) as null_user_ids,\n",
    "    SUM(CASE WHEN email IS NULL THEN 1 ELSE 0 END) as null_emails,\n",
    "    SUM(CASE WHEN created_date IS NULL THEN 1 ELSE 0 END) as null_dates,\n",
    "    MIN(created_date) as earliest_date,\n",
    "    MAX(created_date) as latest_date\n",
    "  FROM catalog.schema.users\n",
    ")\n",
    "SELECT \n",
    "  total_rows,\n",
    "  unique_users,\n",
    "  ROUND(100.0 * unique_users / total_rows, 2) as uniqueness_pct,\n",
    "  null_user_ids,\n",
    "  ROUND(100.0 * null_user_ids / total_rows, 2) as null_user_id_pct,\n",
    "  null_emails,\n",
    "  ROUND(100.0 * null_emails / total_rows, 2) as null_email_pct,\n",
    "  null_dates,\n",
    "  earliest_date,\n",
    "  latest_date,\n",
    "  DATEDIFF(latest_date, earliest_date) as date_range_days\n",
    "FROM table_stats;\n",
    "```\n",
    "\n",
    "## Best Practices\n",
    "\n",
    "1. **Incremental Validation for Large Tables**\n",
    "   * Use partitioning to validate data incrementally\n",
    "   * Focus on recent data for time-series tables\n",
    "   * Sample large tables for exploratory checks\n",
    "\n",
    "2. **Use Delta Lake Features**\n",
    "   * Implement CHECK constraints for critical rules\n",
    "   * Use DESCRIBE DETAIL to track table metrics\n",
    "   * Leverage Delta Lake history for quality trends\n",
    "\n",
    "3. **Document Quality Rules**\n",
    "   * Keep quality rules in version control\n",
    "   * Document business logic behind each rule\n",
    "   * Version your quality checks\n",
    "\n",
    "4. **Automate Monitoring**\n",
    "   * Schedule quality checks as workflows\n",
    "   * Set up alerts for quality threshold violations\n",
    "   * Track quality metrics over time\n",
    "\n",
    "## Common Pitfalls\n",
    "\n",
    "* **Avoid**: Running full table scans on large tables without sampling\n",
    "* **Avoid**: Implementing overly strict constraints that block valid data\n",
    "* **Avoid**: Checking quality only at ingestion (validate throughout pipeline)\n",
    "* **Avoid**: Ignoring data types when checking ranges and formats\n",
    "\n",
    "## Related Documentation\n",
    "\n",
    "* [Delta Lake Constraints](https://docs.databricks.com/delta/constraints.html)\n",
    "* [Data Quality Monitoring](https://docs.databricks.com/lakehouse-monitoring/index.html)\n",
    "* [Databricks SQL Functions](https://docs.databricks.com/sql/language-manual/sql-ref-functions.html)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "99c25ca4-30ec-4c9b-af6a-cd9ab23561e2",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "README.md - Skill Overview"
    }
   },
   "source": [
    "## README.md File\n",
    "\n",
    "The README provides a quick overview and usage instructions.\n",
    "\n",
    "---\n",
    "\n",
    "```markdown\n",
    "# Data Quality Checks Skill\n",
    "\n",
    "## Overview\n",
    "\n",
    "This skill provides comprehensive guidance for implementing data quality validation on Delta tables in Databricks. It includes patterns for completeness checks, duplicate detection, constraint enforcement, and automated monitoring.\n",
    "\n",
    "## Quick Start\n",
    "\n",
    "1. **Load the Skill**: The Assistant will automatically load this skill when you ask about data quality validation\n",
    "2. **Review Patterns**: Check the SKILL.md file for common validation patterns\n",
    "3. **Run Examples**: Use the SQL queries in the examples section\n",
    "4. **Customize**: Adapt the patterns to your specific data quality requirements\n",
    "\n",
    "## What's Included\n",
    "\n",
    "* **SKILL.md**: Main skill documentation with YML frontmatter\n",
    "* **SQL Examples**: Completeness checks, duplicate detection, constraint management\n",
    "* **Best Practices**: Guidelines for implementing quality checks at scale\n",
    "* **Automation Scripts**: Sample workflows for scheduled quality monitoring\n",
    "\n",
    "## Use Cases\n",
    "\n",
    "* Validate data completeness (null checks)\n",
    "* Detect duplicate records\n",
    "* Enforce data constraints (CHECK constraints)\n",
    "* Generate quality reports\n",
    "* Set up automated quality monitoring\n",
    "* Track quality metrics over time\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "* Databricks Runtime 13.3 LTS or higher\n",
    "* Access to Unity Catalog (for constraint management)\n",
    "* SQL Warehouse or Cluster with appropriate permissions\n",
    "\n",
    "## Example Usage\n",
    "\n",
    "### Check for Null Values\n",
    "\n",
    "```sql\n",
    "SELECT \n",
    "  'user_id' as column_name,\n",
    "  COUNT(*) as total_rows,\n",
    "  SUM(CASE WHEN user_id IS NULL THEN 1 ELSE 0 END) as null_count\n",
    "FROM catalog.schema.customers;\n",
    "```\n",
    "\n",
    "### Find Duplicates\n",
    "\n",
    "```sql\n",
    "SELECT \n",
    "  customer_id,\n",
    "  COUNT(*) as duplicate_count\n",
    "FROM catalog.schema.customers\n",
    "GROUP BY customer_id\n",
    "HAVING COUNT(*) > 1;\n",
    "```\n",
    "\n",
    "### Add Quality Constraints\n",
    "\n",
    "```sql\n",
    "ALTER TABLE catalog.schema.customers\n",
    "ADD CONSTRAINT valid_email CHECK (email LIKE '%@%.%');\n",
    "```\n",
    "\n",
    "## File Structure\n",
    "\n",
    "```\n",
    "skills/data-quality-checks/\n",
    "├── SKILL.md                    # Main skill file with YML frontmatter\n",
    "├── README.md                   # This file\n",
    "├── examples/\n",
    "│   ├── completeness-checks.sql # Null value detection queries\n",
    "│   ├── duplicate-detection.sql # Duplicate finding queries\n",
    "│   └── quality-report.sql      # Comprehensive quality reports\n",
    "└── workflows/\n",
    "    └── daily-quality-check.yml # Sample workflow definition\n",
    "```\n",
    "\n",
    "## Related Skills\n",
    "\n",
    "* **data-sampling**: Best practices for sampling data before validation\n",
    "* **writing-sql**: SQL patterns for quality checks\n",
    "* **using-metric-views**: Creating reusable quality metrics\n",
    "\n",
    "## Contributing\n",
    "\n",
    "To improve this skill:\n",
    "1. Add new validation patterns to SKILL.md\n",
    "2. Include SQL examples in the examples/ directory\n",
    "3. Update the version number in the YML frontmatter\n",
    "4. Document any new dependencies or prerequisites\n",
    "\n",
    "## Version History\n",
    "\n",
    "* **1.0.0** (2025-02-10): Initial release with SQL-based quality checks\n",
    "\n",
    "## Support\n",
    "\n",
    "For questions or issues with this skill, contact the Field Engineering team or submit feedback through the Assistant.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1630bb40-eadf-445a-9549-db1ce3e6964b",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Example: Completeness Checks (SQL)"
    }
   },
   "outputs": [],
   "source": [
    "-- examples/completeness-checks.sql\n",
    "-- Check for null values across multiple columns\n",
    "\n",
    "-- Method 1: Individual column checks\n",
    "SELECT \n",
    "  'user_id' as column_name,\n",
    "  COUNT(*) as total_rows,\n",
    "  SUM(CASE WHEN user_id IS NULL THEN 1 ELSE 0 END) as null_count,\n",
    "  ROUND(100.0 * SUM(CASE WHEN user_id IS NULL THEN 1 ELSE 0 END) / COUNT(*), 2) as null_percentage,\n",
    "  CASE \n",
    "    WHEN SUM(CASE WHEN user_id IS NULL THEN 1 ELSE 0 END) = 0 THEN 'PASS'\n",
    "    ELSE 'FAIL'\n",
    "  END as status\n",
    "FROM samples.tpch.customer\n",
    "\n",
    "UNION ALL\n",
    "\n",
    "SELECT \n",
    "  'c_name' as column_name,\n",
    "  COUNT(*) as total_rows,\n",
    "  SUM(CASE WHEN c_name IS NULL THEN 1 ELSE 0 END) as null_count,\n",
    "  ROUND(100.0 * SUM(CASE WHEN c_name IS NULL THEN 1 ELSE 0 END) / COUNT(*), 2) as null_percentage,\n",
    "  CASE \n",
    "    WHEN SUM(CASE WHEN c_name IS NULL THEN 1 ELSE 0 END) = 0 THEN 'PASS'\n",
    "    ELSE 'FAIL'\n",
    "  END as status\n",
    "FROM samples.tpch.customer\n",
    "\n",
    "UNION ALL\n",
    "\n",
    "SELECT \n",
    "  'c_phone' as column_name,\n",
    "  COUNT(*) as total_rows,\n",
    "  SUM(CASE WHEN c_phone IS NULL THEN 1 ELSE 0 END) as null_count,\n",
    "  ROUND(100.0 * SUM(CASE WHEN c_phone IS NULL THEN 1 ELSE 0 END) / COUNT(*), 2) as null_percentage,\n",
    "  CASE \n",
    "    WHEN SUM(CASE WHEN c_phone IS NULL THEN 1 ELSE 0 END) = 0 THEN 'PASS'\n",
    "    ELSE 'FAIL'\n",
    "  END as status\n",
    "FROM samples.tpch.customer;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5c20e5f0-7ff6-4220-a7ee-2add58962185",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Example: Duplicate Detection (SQL)"
    }
   },
   "outputs": [],
   "source": [
    "-- examples/duplicate-detection.sql\n",
    "-- Find duplicate records based on key columns\n",
    "\n",
    "-- Method 1: Simple duplicate count\n",
    "WITH duplicate_summary AS (\n",
    "  SELECT \n",
    "    c_custkey,\n",
    "    COUNT(*) as record_count\n",
    "  FROM samples.tpch.customer\n",
    "  GROUP BY c_custkey\n",
    "  HAVING COUNT(*) > 1\n",
    ")\n",
    "SELECT \n",
    "  COUNT(*) as total_duplicate_groups,\n",
    "  SUM(record_count) as total_duplicate_records\n",
    "FROM duplicate_summary;\n",
    "\n",
    "-- Method 2: Detailed duplicate analysis\n",
    "-- WITH duplicates AS (\n",
    "--   SELECT \n",
    "--     c_custkey,\n",
    "--     c_name,\n",
    "--     c_phone,\n",
    "--     COUNT(*) as duplicate_count,\n",
    "--     ROW_NUMBER() OVER (PARTITION BY c_custkey ORDER BY c_name) as row_num\n",
    "--   FROM samples.tpch.customer\n",
    "--   GROUP BY c_custkey, c_name, c_phone\n",
    "--   HAVING COUNT(*) > 1\n",
    "-- )\n",
    "-- SELECT \n",
    "--   c_custkey,\n",
    "--   c_name,\n",
    "--   c_phone,\n",
    "--   duplicate_count,\n",
    "--   CASE WHEN row_num = 1 THEN 'KEEP' ELSE 'REMOVE' END as recommendation\n",
    "-- FROM duplicates\n",
    "-- ORDER BY duplicate_count DESC, c_custkey;\n",
    "\n",
    "-- Method 3: Find duplicates with all original records\n",
    "-- SELECT \n",
    "--   c.*,\n",
    "--   COUNT(*) OVER (PARTITION BY c.c_custkey) as duplicate_count\n",
    "-- FROM samples.tpch.customer c\n",
    "-- QUALIFY duplicate_count > 1\n",
    "-- ORDER BY c.c_custkey;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "006da279-378a-4474-88a9-c3966c368f56",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Example: Quality Report (SQL)"
    }
   },
   "outputs": [],
   "source": [
    "-- examples/quality-report.sql\n",
    "-- Comprehensive data quality report\n",
    "\n",
    "WITH table_stats AS (\n",
    "  SELECT \n",
    "    COUNT(*) as total_rows,\n",
    "    COUNT(DISTINCT c_custkey) as unique_customers,\n",
    "    \n",
    "    -- Null counts\n",
    "    SUM(CASE WHEN c_custkey IS NULL THEN 1 ELSE 0 END) as null_custkey,\n",
    "    SUM(CASE WHEN c_name IS NULL THEN 1 ELSE 0 END) as null_name,\n",
    "    SUM(CASE WHEN c_phone IS NULL THEN 1 ELSE 0 END) as null_phone,\n",
    "    SUM(CASE WHEN c_acctbal IS NULL THEN 1 ELSE 0 END) as null_balance,\n",
    "    \n",
    "    -- Value ranges\n",
    "    MIN(c_acctbal) as min_balance,\n",
    "    MAX(c_acctbal) as max_balance,\n",
    "    AVG(c_acctbal) as avg_balance,\n",
    "    \n",
    "    -- Data freshness (if date column exists)\n",
    "    CURRENT_DATE() as report_date\n",
    "    \n",
    "  FROM samples.tpch.customer\n",
    ")\n",
    "SELECT \n",
    "  report_date,\n",
    "  total_rows,\n",
    "  unique_customers,\n",
    "  ROUND(100.0 * unique_customers / total_rows, 2) as uniqueness_pct,\n",
    "  \n",
    "  -- Completeness metrics\n",
    "  null_custkey,\n",
    "  ROUND(100.0 * null_custkey / total_rows, 2) as null_custkey_pct,\n",
    "  null_name,\n",
    "  ROUND(100.0 * null_name / total_rows, 2) as null_name_pct,\n",
    "  null_phone,\n",
    "  ROUND(100.0 * null_phone / total_rows, 2) as null_phone_pct,\n",
    "  null_balance,\n",
    "  ROUND(100.0 * null_balance / total_rows, 2) as null_balance_pct,\n",
    "  \n",
    "  -- Value statistics\n",
    "  ROUND(min_balance, 2) as min_balance,\n",
    "  ROUND(max_balance, 2) as max_balance,\n",
    "  ROUND(avg_balance, 2) as avg_balance,\n",
    "  \n",
    "  -- Overall quality score (100 - average null percentage)\n",
    "  ROUND(100 - (\n",
    "    (null_custkey + null_name + null_phone + null_balance) * 100.0 / (total_rows * 4)\n",
    "  ), 2) as overall_quality_score\n",
    "  \n",
    "FROM table_stats;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "46fee7ae-5ac9-4c05-bf84-f963e35b2973",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Example: Constraint Management"
    }
   },
   "source": [
    "## Constraint Management Examples\n",
    "\n",
    "### SQL Examples for Managing Delta Lake CHECK Constraints\n",
    "\n",
    "```sql\n",
    "-- examples/constraint-management.sql\n",
    "-- Managing Delta Lake CHECK constraints for data quality\n",
    "\n",
    "-- Example 1: Add email validation constraint\n",
    "ALTER TABLE catalog.schema.users\n",
    "ADD CONSTRAINT valid_email CHECK (email LIKE '%@%.%');\n",
    "\n",
    "-- Example 2: Add positive amount constraint\n",
    "ALTER TABLE catalog.schema.transactions\n",
    "ADD CONSTRAINT positive_amount CHECK (amount > 0);\n",
    "\n",
    "-- Example 3: Add date range constraint\n",
    "ALTER TABLE catalog.schema.events\n",
    "ADD CONSTRAINT valid_date CHECK (event_date <= current_date());\n",
    "\n",
    "-- Example 4: Add status validation constraint\n",
    "ALTER TABLE catalog.schema.orders\n",
    "ADD CONSTRAINT valid_status \n",
    "CHECK (status IN ('pending', 'processing', 'completed', 'cancelled'));\n",
    "\n",
    "-- Example 5: Add age range constraint\n",
    "ALTER TABLE catalog.schema.customers\n",
    "ADD CONSTRAINT valid_age CHECK (age >= 0 AND age <= 120);\n",
    "\n",
    "-- View existing constraints on a table\n",
    "SHOW TBLPROPERTIES catalog.schema.table;\n",
    "\n",
    "-- Drop a constraint if needed\n",
    "ALTER TABLE catalog.schema.table\n",
    "DROP CONSTRAINT constraint_name;\n",
    "```\n",
    "\n",
    "### Testing Constraints\n",
    "\n",
    "```sql\n",
    "-- Test constraint violation (will fail if constraint exists)\n",
    "-- This INSERT will be rejected if the valid_email constraint is active\n",
    "INSERT INTO catalog.schema.users (id, email) \n",
    "VALUES (1, 'invalid-email');\n",
    "\n",
    "-- This INSERT will succeed\n",
    "INSERT INTO catalog.schema.users (id, email) \n",
    "VALUES (1, 'valid@example.com');\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ebf0796d-1631-4ade-a8f6-697500c6c81f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Workflow Configuration for Automated Quality Checks\n",
    "\n",
    "Example workflow YAML for scheduled data quality monitoring:\n",
    "\n",
    "```yaml\n",
    "# workflows/daily-quality-check.yml\n",
    "# Databricks Workflow for automated data quality monitoring\n",
    "\n",
    "name: Daily Data Quality Checks\n",
    "\n",
    "triggers:\n",
    "  - schedule:\n",
    "      quartz_cron_expression: \"0 0 8 * * ?\"  # Run daily at 8 AM UTC\n",
    "      timezone_id: \"UTC\"\n",
    "      pause_status: \"UNPAUSED\"\n",
    "\n",
    "tasks:\n",
    "  - task_key: completeness_check\n",
    "    description: \"Check for null values in critical columns\"\n",
    "    sql_task:\n",
    "      warehouse_id: \"{{ warehouse_id }}\"\n",
    "      query:\n",
    "        query: |\n",
    "          SELECT \n",
    "            'completeness_check' as check_type,\n",
    "            CURRENT_TIMESTAMP() as check_time,\n",
    "            column_name,\n",
    "            total_rows,\n",
    "            null_count,\n",
    "            null_percentage,\n",
    "            status\n",
    "          FROM (\n",
    "            SELECT \n",
    "              'user_id' as column_name,\n",
    "              COUNT(*) as total_rows,\n",
    "              SUM(CASE WHEN user_id IS NULL THEN 1 ELSE 0 END) as null_count,\n",
    "              ROUND(100.0 * SUM(CASE WHEN user_id IS NULL THEN 1 ELSE 0 END) / COUNT(*), 2) as null_percentage,\n",
    "              CASE WHEN SUM(CASE WHEN user_id IS NULL THEN 1 ELSE 0 END) = 0 THEN 'PASS' ELSE 'FAIL' END as status\n",
    "            FROM catalog.schema.users\n",
    "          )\n",
    "          WHERE status = 'FAIL';\n",
    "    \n",
    "  - task_key: duplicate_check\n",
    "    description: \"Detect duplicate records\"\n",
    "    depends_on:\n",
    "      - task_key: completeness_check\n",
    "    sql_task:\n",
    "      warehouse_id: \"{{ warehouse_id }}\"\n",
    "      query:\n",
    "        query: |\n",
    "          WITH duplicates AS (\n",
    "            SELECT \n",
    "              user_id,\n",
    "              COUNT(*) as duplicate_count\n",
    "            FROM catalog.schema.users\n",
    "            GROUP BY user_id\n",
    "            HAVING COUNT(*) > 1\n",
    "          )\n",
    "          SELECT \n",
    "            'duplicate_check' as check_type,\n",
    "            CURRENT_TIMESTAMP() as check_time,\n",
    "            COUNT(*) as duplicate_groups,\n",
    "            SUM(duplicate_count) as total_duplicates,\n",
    "            CASE WHEN COUNT(*) = 0 THEN 'PASS' ELSE 'FAIL' END as status\n",
    "          FROM duplicates;\n",
    "\n",
    "email_notifications:\n",
    "  on_failure:\n",
    "    - user_name@databricks.com\n",
    "\n",
    "max_concurrent_runs: 1\n",
    "timeout_seconds: 3600\n",
    "\n",
    "tags:\n",
    "  RemoveAfter: \"20260210\"  # Per FE workspace policy\n",
    "  purpose: \"data-quality-monitoring\"\n",
    "```\n",
    "\n",
    "### Deployment Options\n",
    "\n",
    "**Via Databricks UI**:\n",
    "* Navigate to Workflows → Create Job\n",
    "* Add SQL tasks manually\n",
    "* Configure schedule and notifications\n",
    "\n",
    "**Via Databricks CLI**:\n",
    "```bash\n",
    "databricks jobs create --json-file workflows/daily-quality-check.yml\n",
    "```\n",
    "\n",
    "**Best Practices**:\n",
    "* Use **RemoveAfter** tags (FE workspace requirement)\n",
    "* Use shared SQL warehouses (dbdemos-shared-endpoint)\n",
    "* Set auto-stop to 10-30 minutes\n",
    "* Schedule during off-peak hours"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a225e300-9da1-403a-aa69-f694975646a0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## How to Create Your Own Databricks Assistant Skill\n",
    "\n",
    "### Directory Structure\n",
    "\n",
    "```\n",
    "/Workspace/Users/your.email@databricks.com/.assistant/skills/\n",
    "└── your-skill-name/\n",
    "    ├── SKILL.md              # Main skill file with YML frontmatter\n",
    "    ├── README.md             # Quick start guide\n",
    "    ├── examples/\n",
    "    │   ├── example1.sql      # SQL examples\n",
    "    │   └── example2.py       # Python examples\n",
    "    └── workflows/\n",
    "        └── automation.yml    # Workflow configs\n",
    "```\n",
    "\n",
    "### SKILL.md YML Frontmatter Template\n",
    "\n",
    "```yaml\n",
    "---\n",
    "name: your-skill-name              # Lowercase with hyphens\n",
    "version: 1.0.0                     # Semantic versioning\n",
    "description: |                     # Multi-line description\n",
    "  Use this skill when:\n",
    "  - Use case 1\n",
    "  - Use case 2\n",
    "  - Use case 3\n",
    "author: Your Name/Team\n",
    "tags:\n",
    "  - category1\n",
    "  - category2\n",
    "related_skills:\n",
    "  - related-skill-1\n",
    "  - related-skill-2\n",
    "last_updated: 2025-02-10          # YYYY-MM-DD format\n",
    "---\n",
    "```\n",
    "\n",
    "### Key Sections in SKILL.md\n",
    "\n",
    "1. **Scope**: Define when to use this skill\n",
    "2. **Core Principles**: Fundamental concepts\n",
    "3. **Common Patterns**: Code examples with explanations\n",
    "4. **Best Practices**: Guidelines and recommendations\n",
    "5. **Common Pitfalls**: What to avoid\n",
    "6. **Related Documentation**: Links to official docs\n",
    "\n",
    "### README.md Template\n",
    "\n",
    "```markdown\n",
    "# Skill Name\n",
    "\n",
    "## Overview\n",
    "Brief description of the skill.\n",
    "\n",
    "## Quick Start\n",
    "1. When the skill is loaded\n",
    "2. What it provides\n",
    "3. How to use it\n",
    "\n",
    "## What's Included\n",
    "* File 1\n",
    "* File 2\n",
    "\n",
    "## Use Cases\n",
    "* Use case 1\n",
    "* Use case 2\n",
    "\n",
    "## Prerequisites\n",
    "* Requirement 1\n",
    "* Requirement 2\n",
    "\n",
    "## Example Usage\n",
    "Code examples here\n",
    "\n",
    "## Version History\n",
    "* 1.0.0 - Initial release\n",
    "```\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "✅ **Clear Scope**: Define exactly when to use the skill  \n",
    "✅ **Working Examples**: Include tested, runnable code  \n",
    "✅ **Documentation Links**: Reference official Databricks docs  \n",
    "✅ **Version Control**: Update version on changes  \n",
    "✅ **FE Compliance**: Follow workspace policies  \n",
    "✅ **Focused Domain**: Keep skills modular and specific\n",
    "\n",
    "### How the Assistant Uses Skills\n",
    "\n",
    "1. User asks a question matching the skill's description\n",
    "2. Assistant loads SKILL.md using `readSkillFile` tool\n",
    "3. Assistant applies patterns and guidance from the skill\n",
    "4. Assistant adapts examples to user's specific needs\n",
    "5. User receives expert-level domain-specific help\n",
    "\n",
    "### Skill Categories (Examples)\n",
    "\n",
    "* **Data Engineering**: ETL, Delta Lake, Streaming\n",
    "* **Data Quality**: Validation, Monitoring, Constraints\n",
    "* **Analytics**: SQL Patterns, Dashboards, Metrics\n",
    "* **Machine Learning**: Training, MLflow, Features\n",
    "* **Security**: Access Control, Encryption\n",
    "* **Performance**: Optimization, Caching, Partitioning\n",
    "\n",
    "### FE Workspace Compliance\n",
    "\n",
    "When creating skills for FE workspaces:\n",
    "* Use **RemoveAfter** tags on resources\n",
    "* Prefer shared compute (dbdemos-shared-endpoint)\n",
    "* Store in personal folders (/Users/your.email/)\n",
    "* Follow naming conventions (avoid tmp\\_, test\\_)\n",
    "* Clean up temporary resources\n",
    "\n",
    "---\n",
    "\n",
    "## What This Notebook Provides\n",
    "\n",
    "✅ Complete **SKILL.md** with YML frontmatter  \n",
    "✅ Comprehensive **README.md** template  \n",
    "✅ Working **SQL examples** for data quality  \n",
    "✅ **Workflow configuration** for automation  \n",
    "✅ **Best practices** for skill development  \n",
    "✅ **FE workspace compliance** guidelines\n",
    "\n",
    "**Next Steps**: Adapt this template for your domain expertise by modifying the content while keeping the structure!"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "dbe_0c235d96-4bc7-4fb5-b118-17fd1dad0124",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "sql",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "Databricks Assistant Skill",
   "widgets": {}
  },
  "language_info": {
   "name": "sql"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
